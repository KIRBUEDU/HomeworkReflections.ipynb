{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 5 Reflection Question 1 \n",
    "\n",
    "1. # Draw a diagram for the following negative feedback loop:\n",
    "\n",
    "# Sweating causes body temperature to decrease.  High body temperature causes sweating.\n",
    "\n",
    "# A negative feedback loop means that one thing increases another while the second thing decreases the first.\n",
    "\n",
    "# Remember that we are using directed acyclic graphs where two things cannot directly cause each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabefd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [High body temperature] -> [Sweating] -> [Body temperature decreases] -> [Reduced high body temperature]\n",
    "# High body temperature leads to sweating, and then sweating causes a decrease in body temperature, reduces the original high temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bafadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 5 Reflection Question 2 \n",
    "\n",
    "# 2) Describe an example of a positive feedback loop.  This means that one things increases another while the second things also increases the first.\n",
    "\n",
    "# Artic ice melting \n",
    "# When the planet gets warmer through higher global temperature, the artic sea ice melts. Ice has the ability to reflect the sunlight, but due to the fact that there is less ice\n",
    "# Not much sunglight gets reflected; therefore the ocean absorbs the heat. When ocean absorbs so much heat it leads to warming of the ocean\n",
    "# The increased ocean warming leads to more ice melting \n",
    "# [Warming] -> [Ice melts] -> [Less reflection] -> [Ocean absorbs heat] -> [Ocean gets more warmer] -> [More ice melts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 5 Reflection Question 3\n",
    "# Draw a diagram for the following situation:\n",
    "\n",
    "#Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.\n",
    "# Bears eat deer, decreasing their population.\n",
    "# Deer eat flowers, decreasing their population.\n",
    "\n",
    "# [Lightning] -> [Deers] -> [Flowers]\n",
    "# [Lightning] -> [Bears] -> [Deers]\n",
    "# [Lightning] -> [Flower] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb1ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lightning</th>\n",
       "      <th>Bears</th>\n",
       "      <th>Deer</th>\n",
       "      <th>Flowers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.745401</td>\n",
       "      <td>77.872870</td>\n",
       "      <td>100.963227</td>\n",
       "      <td>27.829704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.507143</td>\n",
       "      <td>53.625553</td>\n",
       "      <td>57.055497</td>\n",
       "      <td>79.787214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.319939</td>\n",
       "      <td>64.865665</td>\n",
       "      <td>71.885704</td>\n",
       "      <td>59.664798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.986585</td>\n",
       "      <td>66.495319</td>\n",
       "      <td>77.146079</td>\n",
       "      <td>45.287085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.560186</td>\n",
       "      <td>101.527941</td>\n",
       "      <td>106.967561</td>\n",
       "      <td>3.439307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lightning       Bears        Deer    Flowers\n",
       "0   3.745401   77.872870  100.963227  27.829704\n",
       "1   9.507143   53.625553   57.055497  79.787214\n",
       "2   7.319939   64.865665   71.885704  59.664798\n",
       "3   5.986585   66.495319   77.146079  45.287085\n",
       "4   1.560186  101.527941  106.967561   3.439307"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Week 5 Reflection Question 3\n",
    "\n",
    "# Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "\n",
    "lightning = np.random.uniform(0, 10, n)\n",
    "\n",
    "\n",
    "bears = 100 - 5 * lightning + np.random.normal(0, 5, n)\n",
    "\n",
    "deer = 150 - 8 * lightning - 0.3 * bears + np.random.normal(0, 5, n)\n",
    "\n",
    "\n",
    "flowers = 50 + 6 * lightning - 0.5 * deer + np.random.normal(0, 5, n)\n",
    "\n",
    "\n",
    "bears = np.clip(bears, 0, None)\n",
    "deer = np.clip(deer, 0, None)\n",
    "flowers = np.clip(flowers, 0, None)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Lightning\": lightning,\n",
    "    \"Bears\": bears,\n",
    "    \"Deer\": deer,\n",
    "    \"Flowers\": flowers\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 5 Reflection Question 3\n",
    "\n",
    "# Identify a backdoor path with one or more confounders for the relationship between deer and flowers.\n",
    "\n",
    "# Lightning is the confounder it affects both deer and flowers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aaa653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 5 Reflection Question 4\n",
    "\n",
    "# 4) Draw a diagram for a situation of your own invention.  The diagram should include at least four nodes, one confounder, and one collider.  \n",
    "# Be sure that it is acyclic (no loops).  Which node would say is most like a treatment (X)?  Which is most like an outcome (Y)?\n",
    "\n",
    "\n",
    "# [Solar activity] -> [Increase Carbon Dioxide Emission] -> [Global temperatue goes up]-> [Artic Ice melts] -> [Increase in ocean temperature]\n",
    "# [Solar activity] -> [Global temperatue goes up] -> [Artic Ice melts] -> [Increase in ocean temperature]\n",
    "# There are 4 nodes: Carbon Dioxide Emission, Global temperatue, Artic Ice, Ocean Temperature\n",
    "# Cofounder is the solar activity \n",
    "# Global temperature is the collider\n",
    "# Reducing carbon dioxide emission should be a treatment, this is the starting point \n",
    "# Ocean temperature is the outcome - end result everything affects this outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 6 Reflection Question 1 \n",
    "\n",
    "# What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  \n",
    "# (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  \n",
    "# That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)\n",
    "# Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.\n",
    "# (Please explain / justify this answer, or give a different one if you can think of one.)\n",
    "\n",
    "# Answer We are likely to find the item with the most extreme difference, which may be high simply due to randomness. \n",
    "# Each unit’s outcome includes random noise, not just the true treatment effect. When we pick an outlier, it doesn’t represent the average case — it may just be a fluke. \n",
    "# This can lead to an overestimate of the Marginal Treatment Effect (MTE), making the result biased and misleading. \n",
    "# To make a valid estimate, we need to base it on the true treatment effect, not random extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827dcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 6 Reflection Question 2 \n",
    "\n",
    "# Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.\n",
    "# Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.\n",
    "# (Either code this answer or choose a different one.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2840ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Marginal Treatment Effect (90th percentile): 9.49\n"
     ]
    }
   ],
   "source": [
    "# Week 6 Reflection Question 2  Answer Code \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "n = 1000\n",
    "Y0 = np.random.normal(loc=50, scale=10, size=n)   \n",
    "true_effect = np.random.uniform(0, 10, size=n)    \n",
    "Y1 = Y0 + true_effect + np.random.normal(0, 2, size=n)  \n",
    "\n",
    "\n",
    "individual_effects = Y1 - Y0\n",
    "\n",
    "\n",
    "mte_estimate = np.percentile(individual_effects, 90)\n",
    "\n",
    "print(f\"Estimated Marginal Treatment Effect (90th percentile): {mte_estimate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 7 Reflection Question 1 \n",
    "# Create a linear regression model involving a confounder that is left out of the model.  \n",
    "# Show whether the true correlation between $$X$$ and $$Y$$ is overestimated, underestimated, or neither.  \n",
    "# Explain in words why this is the case for the given coefficients you have chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25755e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficient for X (without W): 3.39\n"
     ]
    }
   ],
   "source": [
    "# Week 7 Reflection Question 1  Answer Code\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "n = 500\n",
    "\n",
    "\n",
    "W = np.random.normal(0, 1, n)\n",
    "\n",
    "\n",
    "X = 0.7 * W + np.random.normal(0, 1, n)\n",
    "\n",
    "\n",
    "Y = 2 * X + 3 * W + np.random.normal(0, 1, n)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "X_reshaped = X.reshape(-1, 1)\n",
    "model = LinearRegression().fit(X_reshaped, Y)\n",
    "\n",
    "\n",
    "coef_X = model.coef_[0]\n",
    "\n",
    "print(f\"Estimated coefficient for X (without W): {coef_X:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 7 Reflection Question 1  Answer\n",
    "# Show whether the true correlation between $$X$$ and $$Y$$ is overestimated, underestimated, or neither.  \n",
    "# Explain in words why this is the case for the given coefficients you have chosen.\n",
    "\n",
    "# When we ran the regression without W (the confounder) the estimated coefficient for X is 3.39 \n",
    "# W is the confounder because it affects both X and Y \n",
    "# X is positively correlated with W, and W also increases Y\n",
    "# When we leave W out of the model, its positive influence on Y gets wrongly credited to X\n",
    "# The effect of X on Y looks too big when we leave out W. That's because W increases both X and Y, so the model thinks X is doing more than it really is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 7 Reflection Question 2 \n",
    "# erform a linear regression analysis in which one of the coefficients is zero, e.g.\n",
    "\n",
    "# W = [noise]\n",
    "# X = [noise]\n",
    "# Y = 2 * X + [noise]\n",
    "\n",
    "# And compute the p-value of a coefficient - in this case, the coefficient of W.  \n",
    "# (This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)\n",
    "# If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)\n",
    "# Run the analysis 1000 times and report the best (smallest) p-value.  \n",
    "# If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3959e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest p-value for W (should be null): 0.00047\n",
      "\n",
      "This p-value is below 0.05 — but W has no real effect on Y.\n",
      "This is a false positive caused by repeating the test many times.\n",
      "It's an example of the multiple comparisons problem.\n"
     ]
    }
   ],
   "source": [
    "# Week 7 Reflection Question 2 Code \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 100           \n",
    "repeats = 1000     \n",
    "p_values = []\n",
    "\n",
    "\n",
    "for _ in range(repeats):\n",
    "   \n",
    "    W = np.random.normal(0, 1, n)\n",
    "    X = np.random.normal(0, 1, n)\n",
    "    noise = np.random.normal(0, 1, n)\n",
    "    Y = 2 * X + noise  \n",
    "\n",
    "    \n",
    "    XW = np.column_stack((X, W))\n",
    "    model = LinearRegression().fit(XW, Y)\n",
    "    y_pred = model.predict(XW)\n",
    "\n",
    "    \n",
    "    residuals = Y - y_pred\n",
    "    dof = n - 3  \n",
    "    mse = np.sum(residuals**2) / dof\n",
    "\n",
    "    \n",
    "    XW_with_intercept = np.column_stack((np.ones(n), XW))\n",
    "    cov_matrix = np.linalg.inv(XW_with_intercept.T @ XW_with_intercept)\n",
    "    var_b = mse * np.diag(cov_matrix)  \n",
    "    se_b = np.sqrt(var_b)             \n",
    "    \n",
    "\n",
    "    t_stat_W = model.coef_[1] / se_b[2]\n",
    "    p_value_W = 2 * (1 - stats.t.cdf(np.abs(t_stat_W), df=dof))\n",
    "    p_values.append(p_value_W)\n",
    "\n",
    "\n",
    "min_p = min(p_values)\n",
    "print(f\"Smallest p-value for W (should be null): {min_p:.5f}\")\n",
    "\n",
    "if min_p < 0.05:\n",
    "    print(\"\\nThis p-value is below 0.05 — but W has no real effect on Y.\")\n",
    "    print(\"This is a false positive caused by repeating the test many times.\")\n",
    "    print(\"It's an example of the multiple comparisons problem.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ea875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 7 Reflection Question 2 Answer \n",
    "# If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)\n",
    "# Run the analysis 1000 times and report the best (smallest) p-value.  \n",
    "# If the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?\n",
    "\n",
    "# Anytime the p-value is less than 0.05 the result is statistically significant, the null hypothesis is true \n",
    "# This is just a probability it does not necessarily mean the coefficient is non - zeo \n",
    "# When a test is run many times the result will appear significant by pure chance. Even if there is no real effect \n",
    "# Picking the smallest p-value from the many runs does not lead to the right trustworthy answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 8 Reflection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0839d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 From Week 8 Quiz \n",
    "# Using homework_8.1.csv, find the Average treatment effect with inverse probability weighting. \n",
    "# Then, include your code and a written explanation of your work (mentioning any choices or strategies you made in writing the code) in your homework reflection.  \n",
    "\n",
    "\n",
    "\n",
    "# Here are some steps to follow: \n",
    "\n",
    "\n",
    "# Estimate the propensity scores using logistic regression. Fit the model so that the Z values predict ﻿X﻿. \n",
    "\n",
    "# Use the model to predict the propensity scores (e.g., using predict_proba if you are using sklearn). \n",
    "\n",
    "# Calculate inverse probability weights (﻿1 over P﻿ for ﻿X equals 1﻿ and ﻿fraction numerator 1 over denominator 1 minus P end fraction﻿ for ﻿X equals 0﻿). \n",
    "\n",
    "# Estimate the average treatment effect (the Y difference between ﻿X equals 1﻿ and ﻿X equals 0﻿, using the appropriate weights for each). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ad037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 2.2743411898510133\n"
     ]
    }
   ],
   "source": [
    "# Code for Question 1 Week 8 Quiz \n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"homework_8.1.csv\")\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(df[['Z']], df['X'])\n",
    "\n",
    "\n",
    "propensity_scores = model.predict_proba(df[['Z']])[:, 1]\n",
    "\n",
    "\n",
    "df['weights'] = np.where(df['X'] == 1,\n",
    "                         1 / propensity_scores,\n",
    "                         1 / (1 - propensity_scores))\n",
    "\n",
    "\n",
    "treated = df[df['X'] == 1]\n",
    "control = df[df['X'] == 0]\n",
    "\n",
    "weighted_mean_treated = np.average(treated['Y'], weights=treated['weights'])\n",
    "weighted_mean_control = np.average(control['Y'], weights=control['weights'])\n",
    "\n",
    "\n",
    "ate = weighted_mean_treated - weighted_mean_control\n",
    "print(\"ATE:\", ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge and Insight for Question 1\n",
    "\n",
    "\n",
    "# I used logistic regression to get propensity scores, then applied inverse probability weighting to estimate the ATE.\n",
    "# The main challenge was making sure the correct probabilities were used. \n",
    "# It showed how weighting helps estimate effects without randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea92cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 From Week 8 Quiz \n",
    "\n",
    "# Using homework_8.2.csv, match all treated items to the single nearest untreated item using the Mahalanobis distance. \n",
    "# (Do this with replacement — the same untreated item can be used again.) \n",
    "\n",
    "\n",
    "\n",
    "# Use the Mahalanobis function from scipy.spatial.distance \n",
    "\n",
    "# For the inverse covariance matrix, use all ﻿Z 1﻿ values and all ﻿Z 2﻿ values, make them into a ﻿2 x N﻿ matrix, find its ﻿2 x 2﻿ covariance, and invert. \n",
    "\n",
    "\n",
    "\n",
    "# Then, the ATE is closest to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573318aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 3.4376789979126094\n"
     ]
    }
   ],
   "source": [
    "# Code for Question 2 Week 8 Quiz \n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from numpy.linalg import inv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"homework_8.2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "treated = df[df['X'] == 1].reset_index(drop=True)\n",
    "untreated = df[df['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "Z = df[['Z1', 'Z2']].values\n",
    "cov_matrix = np.cov(Z.T)\n",
    "inv_cov_matrix = inv(cov_matrix)\n",
    "\n",
    "\n",
    "matched_Y_diffs = []\n",
    "\n",
    "for i in range(len(treated)):\n",
    "    treated_z = treated.loc[i, ['Z1', 'Z2']].values\n",
    "    distances = untreated[['Z1', 'Z2']].apply(\n",
    "        lambda row: mahalanobis(treated_z, row.values, inv_cov_matrix), axis=1)\n",
    "    \n",
    "    \n",
    "    closest_idx = distances.idxmin()\n",
    "    y_diff = treated.loc[i, 'Y'] - untreated.loc[closest_idx, 'Y']\n",
    "    matched_Y_diffs.append(y_diff)\n",
    "\n",
    "\n",
    "ate_mahalanobis = np.mean(matched_Y_diffs)\n",
    "print(\"ATE:\", ate_mahalanobis)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge and Insight for Question 2\n",
    "\n",
    "# Matching treated units to controls using Mahalanobis distance taught me how important it is to account for the covariance between variables like Z1 and Z2. \n",
    "# One challenge was ensuring the covariance matrix was correctly structured and inverted. \n",
    "# Matching with replacement made things easier\n",
    "# ATE Value of 3.4 showed the method worked "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
